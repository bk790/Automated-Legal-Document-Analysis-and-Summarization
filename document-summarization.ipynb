{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7868867,"sourceType":"datasetVersion","datasetId":4616972}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers rouge-score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-09T07:12:25.445154Z","iopub.execute_input":"2025-05-09T07:12:25.445512Z","iopub.status.idle":"2025-05-09T07:12:33.880553Z","shell.execute_reply.started":"2025-05-09T07:12:25.445488Z","shell.execute_reply":"2025-05-09T07:12:33.879415Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## **Import Libraries and Load Models**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch  # Import torch for tensor operations and model inference\nfrom transformers import BartTokenizer, BartForConditionalGeneration, pipeline, BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import classification_report\n\n# Load the BART summarization model\nsummarization_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nsummarization_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n\n# Load a BERT model for classification\nclassification_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nclassification_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Adjust num_labels as needed\n\n# Create pipelines\nsummarization_pipeline = pipeline(\"summarization\", model=summarization_model, tokenizer=summarization_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2025-05-09T07:12:44.357669Z","iopub.execute_input":"2025-05-09T07:12:44.358528Z","iopub.status.idle":"2025-05-09T07:12:47.445842Z","shell.execute_reply.started":"2025-05-09T07:12:44.358498Z","shell.execute_reply":"2025-05-09T07:12:47.445198Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## **Load and Preprocess the Dataset**","metadata":{}},{"cell_type":"code","source":"# Define the dataset paths\ntrain_judgement_path = '/kaggle/input/legal-case-document-summarization/dataset/UK-Abs/train-data/judgement'\ntest_judgement_path = '/kaggle/input/legal-case-document-summarization/dataset/UK-Abs/test-data/judgement'\n\n# Load and preprocess the text files from the training data\njudgement_files_train = os.listdir(train_judgement_path)\n\n# Ensure input text is within the model's maximum length\nmax_input_length = 1024  # BART's maximum input length\n\n# Process a small sample of files from the training data with truncation\nprocessed_texts_sample_train = []\nfor filename in judgement_files_train[:10]:  # Limiting to the first 10 files for summarization\n    file_path = os.path.join(train_judgement_path, filename)\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n            tokenized_text = summarization_tokenizer.encode(text, truncation=True, max_length=max_input_length)\n            decoded_text = summarization_tokenizer.decode(tokenized_text, skip_special_tokens=True)\n            processed_texts_sample_train.append(decoded_text)\n    except Exception as e:\n        print(f\"Error reading file {filename}: {e}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-09T07:12:56.446337Z","iopub.execute_input":"2025-05-09T07:12:56.446988Z","iopub.status.idle":"2025-05-09T07:12:58.220302Z","shell.execute_reply.started":"2025-05-09T07:12:56.446963Z","shell.execute_reply":"2025-05-09T07:12:58.219011Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## **Generate Summaries**","metadata":{}},{"cell_type":"code","source":"# Generate summaries for the loaded and truncated documents\nsummaries = []\nfor text in processed_texts_sample_train:\n    summary = summarization_pipeline(text, max_length=150, min_length=40, do_sample=False)\n    summaries.append(summary[0]['summary_text'])\n\n# Display the summaries\nfor i, summary in enumerate(summaries):\n    print(f\"Document {i+1} Summary:\")\n    print(summary)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-05-09T07:13:10.111350Z","iopub.execute_input":"2025-05-09T07:13:10.112295Z","iopub.status.idle":"2025-05-09T07:15:04.782313Z","shell.execute_reply.started":"2025-05-09T07:13:10.112262Z","shell.execute_reply":"2025-05-09T07:15:04.781448Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Document 1 Summary:\nFrom 4 April 2005 until 3 December 2012, English law provided for the imposition of sentences of imprisonment for public protection. The case is before the Supreme Court as an application for permission to appeal, with the appeal to follow if permission is granted.\n\n\nDocument 2 Summary:\nHMRC claim that Mr Fowler's income from diving engagements is subject to UK taxation. Mr Fowler denies that he is a self employed contractor. The issue depends on how the double taxation treaty between the UK and South Africa applies to a person in his position.\n\n\nDocument 3 Summary:\nThe appeal is the latest to be heard at the Court of Session in Glasgow. The case involves three fields near the village of Killearn. The fields were let out to a farming partnership under separate leases in 1981 and 1983. The trust acquired the fields because of their potential for residential development.\n\n\nDocument 4 Summary:\nThe need for reliable guidance on this issue is growing day by day. Both appellants claim that they have a well founded fear that they would be persecuted if they were to be returned to their home countries. A huge gulf has opened up in attitudes to and understanding of gay persons.\n\n\nDocument 5 Summary:\nTony Nicklinson, Paul Lamb and someone known for the purpose of these proceedings as Martin, each of whom was suffering such a distressing and undignified life that he had long wished to end it. Mr Lamb contends that the law should permit him to seek assistance in killing himself in this country, and, if it does not, it should be changed so as to enable him to do so.\n\n\nDocument 6 Summary:\nWilliam Hugh Lauchlan and Charles Bernard ONeill were found guilty in the High Court of Justiciary at Glasgow of the murder of Mrs Allison McGarrigle between 21 June and 1 September 1997. The charges of which they were convicted in that trial had been separated from a number of charges on the same indictment of or relating to sexual offences against children. They were sentenced to life imprisonment for the murder, with punishment parts of 26 and 30 years respectively, and to concurrent sentences of eight years imprisonment for attempting to defeat the ends of justice.\n\n\nDocument 7 Summary:\nDigital Satellite Warranty Cover Ltd and Bernard Freeman and Michael Anthony John Sullivan trading as Satellite Services were ordered to be wound up. Their appeal was dismissed by the Court of Appeal [2012] Bus LR 990, and in my opinion it should be dismissed here.\n\n\nDocument 8 Summary:\nThe application for planning permission was submitted by the second appellant (CGI) to the local planning authority, the Dover District Council (the Council), on 13 May 2012. The principal elements were: 521 residential units and a 90 apartment retirement village at Farthingloe; 31 residential units at Western Heights; and conversion of the Drop Redoubt into a visitor centre and museum. The application attracted strong support and strong opposition.\n\n\nDocument 9 Summary:\nTravellers set up camp on Forestry Commissions land at Hethfelton. Commission sought an injunction against all the defendants, including those described as All persons currently living on or occupying the claimants land. Recorder declined to grant an injunction on the view that it would be disproportionate. Court of Appeal reversed the recorder on this point and granted an order.\n\n\nDocument 10 Summary:\nBelhaj and Boudchar v Straw and Ministry of Defence v Rahmatullah concern the alleged complicity of United Kingdom authorities and officials in various torts. The torts alleged include unlawful detention and rendition, torture or cruel and inhuman treatment and assault.\n\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## **Classification of Documents**","metadata":{}},{"cell_type":"code","source":"# Convert summaries (or original texts) to tokenized inputs for classification\nclassification_inputs = classification_tokenizer(summaries, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Perform classification\nwith torch.no_grad():\n    outputs = classification_model(**classification_inputs)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n# Map predictions to labels (Assuming binary classification: 0 = 'Type A', 1 = 'Type B')\nlabels = {0: 'Type A', 1: 'Type B'}\npredicted_labels = [labels[pred.item()] for pred in predictions]\n\n# Display the classification results\nfor i, label in enumerate(predicted_labels):\n    print(f\"Document {i+1} Classified as: {label}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-09T07:15:28.736016Z","iopub.execute_input":"2025-05-09T07:15:28.736688Z","iopub.status.idle":"2025-05-09T07:15:29.576406Z","shell.execute_reply.started":"2025-05-09T07:15:28.736660Z","shell.execute_reply":"2025-05-09T07:15:29.575481Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Document 1 Classified as: Type A\nDocument 2 Classified as: Type B\nDocument 3 Classified as: Type B\nDocument 4 Classified as: Type B\nDocument 5 Classified as: Type B\nDocument 6 Classified as: Type B\nDocument 7 Classified as: Type B\nDocument 8 Classified as: Type B\nDocument 9 Classified as: Type B\nDocument 10 Classified as: Type B\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## **Evaluate Classification Performance**","metadata":{}},{"cell_type":"code","source":"# Dummy reference labels (for demonstration purposes)\nreference_labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # Replace with actual labels\n\n# Generate a classification report\nprint(\"Classification Report:\")\nprint(classification_report(reference_labels, predictions, target_names=['Type A', 'Type B']))","metadata":{"execution":{"iopub.status.busy":"2025-05-09T07:15:40.060560Z","iopub.execute_input":"2025-05-09T07:15:40.060882Z","iopub.status.idle":"2025-05-09T07:15:40.076863Z","shell.execute_reply.started":"2025-05-09T07:15:40.060857Z","shell.execute_reply":"2025-05-09T07:15:40.075887Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n      Type A       1.00      0.20      0.33         5\n      Type B       0.56      1.00      0.71         5\n\n    accuracy                           0.60        10\n   macro avg       0.78      0.60      0.52        10\nweighted avg       0.78      0.60      0.52        10\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}